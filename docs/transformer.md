# key concept

1. transformer
   1. just a tool input character and output next character propability
   2. component
      1. encoder
      2. decoder 
   3. theory
      1. attention theory(注意力机制) (Q K V)
         1. attention theory is a way to find the most important information in a sequence, use the most important information to predict the next character
         2. Q is the query, K is the key, V is the value, use Q and K to find the most important information, use V to predict the next character, for example
      2. 自注意力机制是指，确定词在句中的总要程度