# SFT (监督微调)

## 什么是微调？

**监督微调 (Supervised Fine-Tuning, SFT)** 是在你特定的数据上训练现有的预训练模型，使其适应你的领域或任务的过程。它调整模型的参数以专门针对你的用例。

**核心思想**: 采用通用 LLM → 在你的数据上训练 → 获得专用模型

**为什么微调**:
- 提高特定领域任务的性能
- 学习专业术语和模式
- 减少提示工程需求
- 输出的一致性更好

::: info 前端工程师建议
**实施建议：雇佣 ML 工程师**

作为一名前端开发者，除非使用像 OpenAI 的微调 API 这样的托管服务，否则你通常**不**应该自己实施 SFT。这通常是机器学习工程师的领域。
:::

## SFT vs RAG vs 提示工程

| 方面 | 提示工程 | RAG | 微调 (SFT) |
|--------|-------------------|-----|-------------------|
| **成本** | 非常低 (免费) | 低 (推理 + 检索) | 非常高 (GPU 训练) |
| **设置时间** | 分钟 | 数小时到数天 | 数天到数周 |
| **所需数据** | 无 (只需提示词) | 文档用于检索 | 100s-1000s 标注示例 |
| **速度** | 快 | 中 (检索开销) | 快 (训练后) |
| **更新** | 即时 | 即时 (更新知识库) | 需要重新训练 |
| **最适合** | 通用任务 | 动态知识 | 专业领域 |

## 何时使用微调

### ✅ 好的用例

1. **专业领域语言**
   - 医疗诊断 (医学术语)
   - 法律文档分析 (法律术语)

2. **一致的输出格式**
   - 始终需要特定的 JSON 结构
   - 专有框架中的代码生成

3. **风格和语气**
   - 品牌特定的写作风格
   - 跨响应的一致个性

4. **性能优化**
   - 需要具有专门能力的较小模型以降低成本/延迟

### ❌ 坏的用例 (改用 RAG)

1. **经常变化的信息**
   - 产品目录
   - 新闻和更新

2. **大型知识库**
   - 公司 wiki
   - 技术手册

3. **预算有限**
   - 没有 GPU 访问权限的初创公司
   - 原型/MVP 阶段

## 微调服务

如果你决定必须进行 SFT，这些平台提供托管的微调服务，无需管理 GPU 基础设施：

- [OpenAI Fine-Tuning](https://platform.openai.com/docs/guides/fine-tuning) (GPT-3.5, GPT-4)
- [Anthropic Fine-Tuning](https://docs.anthropic.com/en/docs/build-with-claude/fine-tuning-overview) (Claude Haiku)
- [Azure OpenAI Service](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/fine-tuning)
- [Together AI](https://www.together.ai/) (开源模型)
- [Anyscale](https://www.anyscale.com/) (开源模型)

## 下一步

- **评估你是否真的需要 SFT** (先尝试 RAG)
- **如果你必须继续，从 OpenAI 的微调 API 开始**
- **与 ML 工程师合作** 进行复杂的模型训练
