# 2.3 Attention 机制

## 概念解析
[Transformer 架构](https://arxiv.org/abs/1706.03762)之所有能够击败早期的 RNN 等模型成为主流，核心在于它不再是顺序读取文本，而是通过 **Self-Attention（自注意力机制）** 来同时“观察”所有文本。

举个例子：句子“我把它放在桌子上，因为**它**很重”。
人类读到“它”的时候，自动就知道它指代前面的某个物体而非桌子。大模型是通过并行的 Q(Query)、K(Key)、V(Value) 矩阵相乘得出来的：
1. **Query (提问者)**：我是“它”，谁跟我有关系？
2. **Key (回答者标签)**：别的词身上挂着的标签矩阵。
3. **Value (最终内容)**：关联后真正提取出的语义。

通过数学点积和 Softmax 分配概率后，模型发现“它”跟前面那个物体的相关度是 `95%`，于是完美“理解”了指代关系。

## 运行示例

进入根目录（`ppts/vibe-coding/examples`），运行：
```bash
npm run demo:2.3
```

## 配置步骤

### Cursor IDE
1. 打开 Cursor Composer (Cmd+I)
2. 输入任务描述
3. 观察 AI 如何处理多文件上下文

### Claude Code
1. 运行 `claude`
2. 使用 `@` 引用多个文件
3. 观察 AI 如何基于 Attention 机制理解跨文件关联

## 核心要点
* 自注意力允许平行阅读全句。
* 每一个词生成时，都会动态感知上下文的不同重点部分。
