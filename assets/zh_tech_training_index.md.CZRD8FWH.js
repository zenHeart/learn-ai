import{_ as e}from"./chunks/plugin-vue_export-helper.DlAUqK2U.js";import{C as o,c as n,o as r,ad as a,b as s,w as l,a as h,G as d,ae as A}from"./chunks/mermaid.Cb40Nz1P.js";import"./chunks/cytoscape.C2CwDKBM.js";import"./chunks/dayjs.C32PoDnw.js";const q=JSON.parse('{"title":"面向前段工程师的 AI 模型训练","description":"","frontmatter":{},"headers":[],"relativePath":"zh/tech/training/index.md","filePath":"zh/tech/training/index.md"}'),g={name:"zh/tech/training/index.md"};function p(u,t,E,c,f,b){const i=o("Mermaid");return r(),n("div",null,[t[1]||(t[1]=a('<h1 id="面向前段工程师的-ai-模型训练" tabindex="-1">面向前段工程师的 AI 模型训练 <a class="header-anchor" href="#面向前段工程师的-ai-模型训练" aria-label="Permalink to &quot;面向前段工程师的 AI 模型训练&quot;">​</a></h1><p><strong>重要提示</strong>: 本节仅<strong>从高层次</strong>解释训练概念。作为一名前端工程师，你很少需要自己实施训练。</p><h2 id="黄金法则" tabindex="-1">黄金法则 <a class="header-anchor" href="#黄金法则" aria-label="Permalink to &quot;黄金法则&quot;">​</a></h2><blockquote><p><strong>99% 的前端 AI 用例应该使用 RAG，而不是训练。</strong></p></blockquote><p>如果你正在考虑模型训练，请问自己：</p><ul><li>我有 10,000+ 个高质量的标注示例吗？</li><li>我有 $10,000+ 的预算用于训练和实验吗？</li><li>我有机器学习工程专业知识或能雇佣到相关人员吗？</li></ul><p><strong>如果你对其中任何一个回答“不”</strong>，请改用 RAG。</p><hr><h2 id="何时使用什么-决策树" tabindex="-1">何时使用什么：决策树 <a class="header-anchor" href="#何时使用什么-决策树" aria-label="Permalink to &quot;何时使用什么：决策树&quot;">​</a></h2>',9)),(r(),s(A,null,{default:l(()=>[d(i,{id:"mermaid-41",class:"mermaid",graph:"graph%20TD%0A%20%20%20%20Start%5B%E6%88%91%E9%9C%80%E8%A6%81%20AI...%5D%20--%3E%20Q1%7B%E4%BD%BF%E7%94%A8%E6%88%91%E7%9A%84%E7%A7%81%E6%9C%89%E6%95%B0%E6%8D%AE%3F%7D%0A%0A%20%20%20%20Q1%20--%3E%7C%E6%98%AF%7C%20RAG%5B%E4%BD%BF%E7%94%A8%20RAG%5D%0A%20%20%20%20Q1%20--%3E%7C%E5%90%A6%7C%20Q2%7B%E6%94%B9%E5%8F%98%20AI%20%E8%A1%8C%E4%B8%BA%3F%7D%0A%0A%20%20%20%20Q2%20--%3E%7C%E9%A3%8E%E6%A0%BC%2F%E8%AF%AD%E6%B0%94%2F%E6%A0%BC%E5%BC%8F%7C%20Prompt%5B%E9%AB%98%E7%BA%A7%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%5D%0A%20%20%20%20Q2%20--%3E%7C%E6%B7%B1%E5%BA%A6%E7%9F%A5%E8%AF%86%7C%20Q3%7B%E6%9C%89%2010k%2B%20%E7%A4%BA%E4%BE%8B%3F%7D%0A%0A%20%20%20%20Q3%20--%3E%7C%E5%90%A6%7C%20RAG2%5B%E4%BB%8D%E7%84%B6%E4%BD%BF%E7%94%A8%20RAG%5D%0A%20%20%20%20Q3%20--%3E%7C%E6%98%AF%7C%20Q4%7B%E6%9C%89%20ML%20%E5%9B%A2%E9%98%9F%3F%7D%0A%0A%20%20%20%20Q4%20--%3E%7C%E5%90%A6%7C%20Hire%5B%E9%9B%87%E4%BD%A3%20ML%20%E5%B7%A5%E7%A8%8B%E5%B8%88%5D%0A%20%20%20%20Q4%20--%3E%7C%E6%98%AF%7C%20Train%5B%E8%80%83%E8%99%91%E8%AE%AD%E7%BB%83%5D%0A%0A%20%20%20%20Train%20--%3E%20SFT%5BSFT%20%E7%94%A8%E4%BA%8E%E6%96%B0%E7%9F%A5%E8%AF%86%5D%0A%20%20%20%20Train%20--%3E%20RLHF%5BRLHF%20%E7%94%A8%E4%BA%8E%E8%A1%8C%E4%B8%BA%5D%0A%20%20%20%20Train%20--%3E%20PEFT%5BPEFT%20%E8%8A%82%E7%9C%81%E6%88%90%E6%9C%AC%5D%0A%0A%20%20%20%20style%20RAG%20fill%3A%23d4edda%0A%20%20%20%20style%20RAG2%20fill%3A%23d4edda%0A%20%20%20%20style%20Prompt%20fill%3A%23d4edda%0A%20%20%20%20style%20Hire%20fill%3A%23fff3cd%0A%20%20%20%20style%20Train%20fill%3A%23f8d7da%0A"})]),fallback:l(()=>[...t[0]||(t[0]=[h(" Loading... ",-1)])]),_:1})),t[2]||(t[2]=a('<hr><h2 id="训练方法比较" tabindex="-1">训练方法比较 <a class="header-anchor" href="#训练方法比较" aria-label="Permalink to &quot;训练方法比较&quot;">​</a></h2><table tabindex="0"><thead><tr><th>方法</th><th>作用</th><th>公司何时使用</th><th>前端相关性</th><th>成本</th></tr></thead><tbody><tr><td><strong><a href="/learn-ai/zh/tech/patterns/RAG.html">RAG</a></strong></td><td>检索文档 + 注入上下文</td><td>总是 (首选)</td><td>✅ <strong>你来实现这个</strong></td><td>$</td></tr><tr><td><strong><a href="/learn-ai/zh/tech/training/SFT.html">SFT</a></strong></td><td>通过示例教授新知识</td><td>自定义领域 (法律, 医疗)</td><td>❌ 雇佣 ML 工程师</td><td>$$$</td></tr><tr><td><strong>RLHF</strong> 🚧</td><td>通过人类反馈完善行为</td><td>ChatGPT 风格的对齐</td><td>❌ 仅限研究团队</td><td>$$$$</td></tr><tr><td><strong>PEFT</strong> 🚧</td><td>高效微调 (LoRA)</td><td>预算敏感的训练</td><td>❌ ML 工程师实现</td><td>$$</td></tr></tbody></table><p>🚧 = 仅概念文档 (即将推出)</p><hr><h2 id="为什么-rag-是你的朋友" tabindex="-1">为什么 RAG 是你的朋友 <a class="header-anchor" href="#为什么-rag-是你的朋友" aria-label="Permalink to &quot;为什么 RAG 是你的朋友&quot;">​</a></h2><p><strong>RAG (检索增强生成)</strong> 解决了 99% 的“我需要 AI 知道我的数据”的问题：</p><p>✅ <strong>适用于任何 LLM</strong> - 无需训练 ✅ <strong>即时更新</strong> - 添加新文档，立即可用 ✅ <strong>成本极低</strong> - 向量搜索 + API 调用 vs. 数千美元的训练 ✅ <strong>可调试</strong> - 确切看到检索了哪些文档 ✅ <strong>前端友好</strong> - 你控制整个栈</p><p><strong>RAG 处理的示例用例</strong>:</p><ul><li>产品文档的客户支持</li><li>内部知识库搜索</li><li>代码文档问答</li><li>法律/医疗文档分析</li></ul><p><strong>阅读更多</strong>: <a href="/learn-ai/zh/tech/patterns/RAG.html">RAG 完整指南</a></p><hr><h2 id="训练真正有意义的时候" tabindex="-1">训练真正有意义的时候 <a class="header-anchor" href="#训练真正有意义的时候" aria-label="Permalink to &quot;训练真正有意义的时候&quot;">​</a></h2><p>只有当你满足<strong>所有</strong>这些标准时，才应考虑训练：</p><h3 id="对于监督微调-sft" tabindex="-1">对于监督微调 (SFT) <a class="header-anchor" href="#对于监督微调-sft" aria-label="Permalink to &quot;对于监督微调 (SFT)&quot;">​</a></h3><ul><li>[ ] 你需要 AI 学习公共训练数据中不存在的<strong>特定领域知识</strong></li><li>[ ] 你有 <strong>10,000+ 个高质量的标注示例</strong> (不仅仅是文档 - 而是实际的输入/输出对)</li><li>[ ] RAG 对于你的用例来说太慢或太贵</li><li>[ ] 你有 <strong>$5,000-$50,000</strong> 的训练成本预算</li><li>[ ] 你有可以实施和调试此过程的 ML 工程师</li></ul><p><strong>示例</strong>: 一家医疗公司使用 50,000 个带注释的示例训练 LLM 来解释专有的实验室报告。</p><h3 id="对于-rlhf-基于人类反馈的强化学习" tabindex="-1">对于 RLHF (基于人类反馈的强化学习) <a class="header-anchor" href="#对于-rlhf-基于人类反馈的强化学习" aria-label="Permalink to &quot;对于 RLHF (基于人类反馈的强化学习)&quot;">​</a></h3><ul><li>[ ] 你需要改变<strong>行为</strong> (语气, 风格, 安全性) 而不是增加知识</li><li>[ ] 你有数百小时的人类反馈数据</li><li>[ ] 你有具备 RLHF 专业知识的研究团队</li><li>[ ] 预算: <strong>$50,000+</strong></li></ul><p><strong>示例</strong>: OpenAI 对齐 GPT-4 以拒绝有害请求。</p><h3 id="对于-peft-参数高效微调" tabindex="-1">对于 PEFT (参数高效微调) <a class="header-anchor" href="#对于-peft-参数高效微调" aria-label="Permalink to &quot;对于 PEFT (参数高效微调)&quot;">​</a></h3><ul><li>[ ] 你已经在做 SFT 但需要降低成本</li><li>[ ] 你需要部署多个专用模型</li><li>[ ] 你有熟悉 LoRA/QLoRA 的 ML 工程师</li></ul><p><strong>示例</strong>: 初创公司创建客户特定的模型变体，而无需全量微调成本。</p><hr><h2 id="何时雇佣-ml-工程师-清单" tabindex="-1">&quot;何时雇佣 ML 工程师&quot; 清单 <a class="header-anchor" href="#何时雇佣-ml-工程师-清单" aria-label="Permalink to &quot;&quot;何时雇佣 ML 工程师&quot; 清单&quot;">​</a></h2><p>如果你是前端工程师并对其中任何一个回答“是”，那么是时候让 ML 专家参与进来了：</p><ul><li>[ ] 即使经过优化，RAG 的准确性也不够</li><li>[ ] 你需要模型“记住”领域知识 (SFT)</li><li>[ ] 你每月的 API 调用花费超过 $10k，训练可能会降低成本</li><li>[ ] 你需要在设备上或受限环境中部署模型</li><li>[ ] 你的公司想要专有的 AI 优势 (自定义模型)</li></ul><p><strong>你应该先做什么</strong>:</p><ol><li>穷尽所有 RAG 优化 (混合搜索, 重排序, 更好的分块)</li><li>尝试高级提示技术 (few-shot, chain-of-thought)</li><li>测试多个 LLM 提供商 (Claude, GPT-4, Gemini)</li><li>计算 ROI: 训练节省的成本会超过 $50k+ 的实施成本吗？</li></ol><hr><h2 id="训练概念概览" tabindex="-1">训练概念概览 <a class="header-anchor" href="#训练概念概览" aria-label="Permalink to &quot;训练概念概览&quot;">​</a></h2><h3 id="监督微调-sft" tabindex="-1">监督微调 (SFT) <a class="header-anchor" href="#监督微调-sft" aria-label="Permalink to &quot;监督微调 (SFT)&quot;">​</a></h3><p><strong>是什么</strong>: 在输入-输出示例上训练模型以教授新知识或任务。</p><p><strong>优点</strong>:</p><ul><li>模型“学习”领域特定知识</li><li>在狭窄领域可以比 RAG 提高准确性</li><li>响应可能更快 (无检索步骤)</li></ul><p><strong>缺点</strong>:</p><ul><li>需要 1,000-100,000+ 个示例</li><li>昂贵 ($1k-$100k 取决于规模)</li><li>静态 - 新数据需要重新训练</li><li>出错时难以调试</li></ul><p><strong>阅读更多</strong>: <a href="/learn-ai/zh/tech/training/SFT.html">SFT 指南</a></p><hr><h3 id="rlhf-基于人类反馈的强化学习" tabindex="-1">RLHF (基于人类反馈的强化学习) <a class="header-anchor" href="#rlhf-基于人类反馈的强化学习" aria-label="Permalink to &quot;RLHF (基于人类反馈的强化学习)&quot;">​</a></h3><p><strong>是什么</strong>: 人类标注者对模型输出进行排名 (好/坏)，模型学习偏好“好”的响应。</p><p><strong>使用者</strong>: ChatGPT, Claude, Gemini (所有主要 LLM)</p><p><strong>前端相关性</strong>: 你永远不会实施这个。这是 OpenAI/Anthropic 创建你通过 API 使用的安全、有用的模型的方式。</p><p><strong>仅概念文档即将推出</strong></p><hr><h3 id="peft-参数高效微调" tabindex="-1">PEFT (参数高效微调) <a class="header-anchor" href="#peft-参数高效微调" aria-label="Permalink to &quot;PEFT (参数高效微调)&quot;">​</a></h3><p><strong>是什么</strong>: 像 LoRA 这样的技术，只训练模型参数的一小部分，而不是整个模型。</p><p><strong>为什么存在</strong>: 全量微调 GPT-3.5 成本 $10k+。LoRA 可以用 $100 完成。</p><p><strong>公司何时使用</strong>:</p><ul><li>创建客户特定的模型变体</li><li>预算有限的快速实验</li><li>部署多个专用模型</li></ul><p><strong>前端相关性</strong>: 如果你的公司有 ML 工程师做 SFT，他们可能会使用 PEFT 来降低成本。无论如何，你都将通过 API 使用模型。</p><p><strong>仅概念文档即将推出</strong></p><hr><h2 id="训练提供商-如果你必须的话" tabindex="-1">训练提供商 (如果你必须的话) <a class="header-anchor" href="#训练提供商-如果你必须的话" aria-label="Permalink to &quot;训练提供商 (如果你必须的话)&quot;">​</a></h2><p>如果你决定必须进行训练，这些提供商提供无代码/低代码解决方案：</p><table tabindex="0"><thead><tr><th>提供商</th><th>最适合</th><th>起始成本</th></tr></thead><tbody><tr><td><strong>OpenAI Fine-Tuning</strong></td><td>GPT-3.5/4 微调</td><td>~$1,200 (训练) + 使用费</td></tr><tr><td><strong>Anthropic</strong></td><td>Claude 微调 (可用性有限)</td><td>联系销售</td></tr><tr><td><strong>HuggingFace AutoTrain</strong></td><td>开源模型 (Llama, Mistral)</td><td>$100-$5,000</td></tr><tr><td><strong>Replicate</strong></td><td>一键微调</td><td>每次训练 $0.50-$5</td></tr></tbody></table><p><strong>注意</strong>: 即使使用这些工具，你也需要 ML 专业知识来：</p><ul><li>准备高质量的训练数据</li><li>评估模型性能</li><li>调试问题</li><li>决定何时停止训练</li></ul><hr><h2 id="前端工程师的训练思维模式" tabindex="-1">前端工程师的训练思维模式 <a class="header-anchor" href="#前端工程师的训练思维模式" aria-label="Permalink to &quot;前端工程师的训练思维模式&quot;">​</a></h2><p><strong>你的角色</strong>: 了解这些概念的存在，以便你可以：</p><ul><li>与 ML 团队进行知情对话</li><li>做出自建与购买的决定</li><li>知道何时 RAG 就足够了 (99% 的时间)</li><li>识别何时升级给专家</li></ul><p><strong>你的工作不是</strong>:</p><ul><li>实施训练管道</li><li>调试梯度下降</li><li>选择超参数</li><li>管理 GPU 基础设施</li></ul><p><strong>你的工作是</strong>:</p><ul><li>构建出色的 RAG 系统</li><li>精通提示工程</li><li>通过 API 集成训练好的模型</li><li>为 AI 功能创建 UX</li></ul><hr><h2 id="下一步" tabindex="-1">下一步 <a class="header-anchor" href="#下一步" aria-label="Permalink to &quot;下一步&quot;">​</a></h2><h3 id="对于大多数前端工程师" tabindex="-1">对于大多数前端工程师 <a class="header-anchor" href="#对于大多数前端工程师" aria-label="Permalink to &quot;对于大多数前端工程师&quot;">​</a></h3><ol><li>精通 RAG: <a href="/learn-ai/zh/tech/patterns/RAG.html">RAG 完整指南</a></li><li>优化提示词: <a href="/learn-ai/zh/tech/prompt/">提示工程</a></li><li>学习结构化输出: <a href="/learn-ai/tech/structured-output.html">类型安全 AI</a> 🚧</li></ol><h3 id="如果你正在考虑训练" tabindex="-1">如果你正在考虑训练 <a class="header-anchor" href="#如果你正在考虑训练" aria-label="Permalink to &quot;如果你正在考虑训练&quot;">​</a></h3><ol><li>阅读 <a href="/learn-ai/zh/tech/training/SFT.html">SFT 指南</a> 了解要求</li><li>审计你的数据: 你有 10k+ 高质量示例吗？</li><li>计算成本: 比较 RAG 优化与训练投资</li><li>在承诺之前咨询 ML 工程师</li></ol><h3 id="如果你正在构建-ai-产品" tabindex="-1">如果你正在构建 AI 产品 <a class="header-anchor" href="#如果你正在构建-ai-产品" aria-label="Permalink to &quot;如果你正在构建 AI 产品&quot;">​</a></h3><ol><li>总是从 RAG 开始</li><li>测量: 优化后 RAG 准确率是否 &lt;90%?</li><li>探索: 更好的提示词/模型能解决吗？</li><li>只有那时: 考虑与 ML 团队一起训练</li></ol><hr><h2 id="总结" tabindex="-1">总结 <a class="header-anchor" href="#总结" aria-label="Permalink to &quot;总结&quot;">​</a></h2><ul><li><strong>99% 的用例 → 使用 RAG</strong> (无需训练)</li><li><strong>训练很昂贵</strong> ($5k-$100k) 且需要 ML 专业知识</li><li><strong>前端工程师</strong> 应该理解概念，而不是实施训练</li><li><strong>如果有疑问</strong>，在考虑训练之前优化 RAG</li></ul><p><strong>黄金法则</strong>: 如果你可以用 RAG 解决，不要训练。如果你可以用更好的提示词解决，不要用 RAG。</p><hr><h2 id="额外资源" tabindex="-1">额外资源 <a class="header-anchor" href="#额外资源" aria-label="Permalink to &quot;额外资源&quot;">​</a></h2><ul><li><a href="https://www.pinecone.io/learn/rag-vs-finetuning/" target="_blank" rel="noreferrer">RAG vs 微调: 何时使用哪个</a> (外部)</li><li><a href="https://platform.openai.com/docs/guides/fine-tuning" target="_blank" rel="noreferrer">OpenAI 微调指南</a> (外部)</li><li><a href="https://www.anthropic.com/research/economics" target="_blank" rel="noreferrer">LLM 微调的经济学</a> (外部 - 示例)</li></ul><p><strong>有问题？</strong> 如果你不确定是使用 RAG 还是训练，<a href="https://github.com/zenheart/learn-ai/discussions" target="_blank" rel="noreferrer">发起讨论</a>。</p>',82))])}const P=e(g,[["render",p]]);export{q as __pageData,P as default};
