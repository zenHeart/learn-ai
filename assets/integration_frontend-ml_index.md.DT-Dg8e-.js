import{_ as e}from"./chunks/plugin-vue_export-helper.DlAUqK2U.js";import{c as r,o as n,ad as a}from"./chunks/mermaid.Cb40Nz1P.js";import"./chunks/cytoscape.C2CwDKBM.js";import"./chunks/dayjs.C32PoDnw.js";const u=JSON.parse('{"title":"Frontend Machine Learning Libraries","description":"","frontmatter":{},"headers":[],"relativePath":"integration/frontend-ml/index.md","filePath":"integration/frontend-ml/index.md"}'),i={name:"integration/frontend-ml/index.md"};function s(o,t,l,d,g,m){return n(),r("div",null,[...t[0]||(t[0]=[a('<h1 id="frontend-machine-learning-libraries" tabindex="-1">Frontend Machine Learning Libraries <a class="header-anchor" href="#frontend-machine-learning-libraries" aria-label="Permalink to &quot;Frontend Machine Learning Libraries&quot;">​</a></h1><p>Running ML in the browser is becoming standard. Here are the major players.</p><table tabindex="0"><thead><tr><th style="text-align:left;">Library</th><th style="text-align:left;">Best For</th><th style="text-align:left;">Size</th><th style="text-align:left;">Complexity</th></tr></thead><tbody><tr><td style="text-align:left;"><strong>Transformers.js</strong></td><td style="text-align:left;">NLP, LLMs, Audio</td><td style="text-align:left;">Medium</td><td style="text-align:left;">Low (Easy API)</td></tr><tr><td style="text-align:left;"><strong>TensorFlow.js</strong></td><td style="text-align:left;">Training, Custom Models</td><td style="text-align:left;">Large</td><td style="text-align:left;">High</td></tr><tr><td style="text-align:left;"><strong>ONNX Runtime Web</strong></td><td style="text-align:left;">Production, Optimization</td><td style="text-align:left;">Small</td><td style="text-align:left;">Medium</td></tr><tr><td style="text-align:left;"><strong>MediaPipe</strong></td><td style="text-align:left;">Vision, Pose, Face</td><td style="text-align:left;">Small</td><td style="text-align:left;">Low</td></tr></tbody></table><h2 id="decision-matrix" tabindex="-1">Decision Matrix <a class="header-anchor" href="#decision-matrix" aria-label="Permalink to &quot;Decision Matrix&quot;">​</a></h2><ul><li><strong>Need to run a HuggingFace model?</strong> -&gt; Use <strong>Transformers.js</strong>.</li><li><strong>Need to train a model in the browser?</strong> -&gt; Use <strong>TensorFlow.js</strong>.</li><li><strong>Need specific computer vision (Hand tracking)?</strong> -&gt; Use <strong>MediaPipe</strong>.</li><li><strong>Need maximum speed for a custom model?</strong> -&gt; Use <strong>ONNX Runtime + WebGPU</strong>.</li></ul><h2 id="performance-webgpu-vs-wasm" tabindex="-1">Performance (WebGPU vs WASM) <a class="header-anchor" href="#performance-webgpu-vs-wasm" aria-label="Permalink to &quot;Performance (WebGPU vs WASM)&quot;">​</a></h2><p>Modern libraries use <strong>WebAssembly (WASM)</strong> for CPU execution and <strong>WebGPU</strong> for GPU execution. WebGPU is 10-100x faster for large matrix multiplications (like LLMs).</p><h2 id="next-steps" tabindex="-1">Next Steps <a class="header-anchor" href="#next-steps" aria-label="Permalink to &quot;Next Steps&quot;">​</a></h2><ul><li><strong><a href="./transformersjs.html">Transformers.js Guide</a></strong> (Recommended start)</li><li><strong><a href="./tensorflowjs.html">TensorFlow.js Guide</a></strong></li><li><strong><a href="./onnx-runtime.html">ONNX Guide</a></strong></li></ul>',9)])])}const p=e(i,[["render",s]]);export{u as __pageData,p as default};
