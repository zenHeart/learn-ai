import{_ as t}from"./chunks/plugin-vue_export-helper.DlAUqK2U.js";import{c as a,o as e,ad as n}from"./chunks/mermaid.Cb40Nz1P.js";import"./chunks/cytoscape.C2CwDKBM.js";import"./chunks/dayjs.C32PoDnw.js";const u=JSON.parse('{"title":"RLHF (基于人类反馈的强化学习)","description":"","frontmatter":{},"headers":[],"relativePath":"zh/tech/training/RLHF.md","filePath":"zh/tech/training/RLHF.md"}'),o={name:"zh/tech/training/RLHF.md"};function i(l,r,s,h,c,g){return e(),a("div",null,[...r[0]||(r[0]=[n('<h1 id="rlhf-基于人类反馈的强化学习" tabindex="-1">RLHF (基于人类反馈的强化学习) <a class="header-anchor" href="#rlhf-基于人类反馈的强化学习" aria-label="Permalink to &quot;RLHF (基于人类反馈的强化学习)&quot;">​</a></h1><h2 id="什么是-rlhf" tabindex="-1">什么是 RLHF？ <a class="header-anchor" href="#什么是-rlhf" aria-label="Permalink to &quot;什么是 RLHF？&quot;">​</a></h2><p><strong>基于人类反馈的强化学习 (Reinforcement Learning from Human Feedback, RLHF)</strong> 是一种用于使语言模型与人类价值观和偏好保持一致的训练技术。它是将原始 GPT-3 变成我们今天使用的有用的 ChatGPT 助手的“秘方”。</p><p><strong>核心思想</strong>: 根据人类对 AI 输出的排名训练奖励模型，然后使用该奖励模型微调 LLM 以生成“更好”的答案。</p><p><strong>RLHF 的三个步骤</strong>:</p><ol><li><strong>监督微调 (SFT)</strong>: 在人类演示上训练模型。</li><li><strong>奖励建模</strong>: 人类对多个模型输出进行排名；训练一个“奖励模型”来预测这些排名。</li><li><strong>强化学习 (PPO)</strong>: 优化 LLM 以最大化来自奖励模型的分数。</li></ol><div class="info custom-block"><p class="custom-block-title">前端相关性</p><p><strong>你不会实施这个</strong></p><p>RLHF 极其昂贵且复杂。它需要海量的人类反馈数据集和大量的 GPU 资源。作为一名前端工程师，你只需要知道<strong>这就是为什么模型拒绝回答有害问题</strong>，或者试图变得“有用、诚实和无害”的原因。</p></div><h2 id="使用者" tabindex="-1">使用者 <a class="header-anchor" href="#使用者" aria-label="Permalink to &quot;使用者&quot;">​</a></h2><p>几乎所有顶级聊天模型都使用 RLHF 或类似的对齐技术（如 RLAIF - AI 反馈）：</p><ul><li><strong>OpenAI</strong>: ChatGPT (GPT-3.5, GPT-4)</li><li><strong>Anthropic</strong>: Claude (Constitutional AI, RLHF 的变体)</li><li><strong>Meta</strong>: Llama 2 &amp; 3 (Llama-2-chat)</li><li><strong>Google</strong>: Gemini</li></ul><h2 id="为什么它对应用开发很重要" tabindex="-1">为什么它对应用开发很重要 <a class="header-anchor" href="#为什么它对应用开发很重要" aria-label="Permalink to &quot;为什么它对应用开发很重要&quot;">​</a></h2><p>理解 RLHF 有助于解释某些模型行为：</p><ol><li><strong>拒绝</strong>: 由于在 RLHF 期间对安全数据过度优化，模型可能会拒绝良性请求。</li><li><strong>冗长</strong>: 由于 RLHF 模式，模型经常变得“话痨”或对答案进行对冲（“作为一门 AI 语言模型...”）。</li><li><strong>风格</strong>: 模型的“个性”很大程度上是在这个阶段形成的。</li></ol><h2 id="延伸阅读" tabindex="-1">延伸阅读 <a class="header-anchor" href="#延伸阅读" aria-label="Permalink to &quot;延伸阅读&quot;">​</a></h2><ul><li><a href="https://openai.com/research/learning-from-human-preferences" target="_blank" rel="noreferrer">Deep reinforcement learning from human preferences (OpenAI)</a></li><li><a href="https://arxiv.org/abs/2203.02155" target="_blank" rel="noreferrer">Training language models to follow instructions with human feedback (InstructGPT Paper)</a></li><li><a href="https://arxiv.org/abs/2212.08073" target="_blank" rel="noreferrer">Constitutional AI: Harmlessness from AI Feedback (Anthropic)</a></li></ul>',15)])])}const L=t(o,[["render",i]]);export{u as __pageData,L as default};
