import{_ as i}from"./chunks/plugin-vue_export-helper.DlAUqK2U.js";import{c as a,o as e,ad as n}from"./chunks/mermaid.Cb40Nz1P.js";import"./chunks/cytoscape.C2CwDKBM.js";import"./chunks/dayjs.C32PoDnw.js";const u=JSON.parse('{"title":"Hugging Face Integration","description":"","frontmatter":{},"headers":[],"relativePath":"integration/apis/huggingface.md","filePath":"integration/apis/huggingface.md"}'),t={name:"integration/apis/huggingface.md"};function l(h,s,r,p,k,o){return e(),a("div",null,[...s[0]||(s[0]=[n(`<h1 id="hugging-face-integration" tabindex="-1">Hugging Face Integration <a class="header-anchor" href="#hugging-face-integration" aria-label="Permalink to &quot;Hugging Face Integration&quot;">​</a></h1><p>Hugging Face is the &quot;GitHub of AI,&quot; hosting over 500,000 open-source models. You can run these models via their <strong>Serverless Inference API</strong> or locally in the browser.</p><h2 id="serverless-inference-api" tabindex="-1">Serverless Inference API <a class="header-anchor" href="#serverless-inference-api" aria-label="Permalink to &quot;Serverless Inference API&quot;">​</a></h2><p>The easiest way to use open models (like Llama 3, Mistral, Bert) without managing servers.</p><h3 id="setup" tabindex="-1">Setup <a class="header-anchor" href="#setup" aria-label="Permalink to &quot;Setup&quot;">​</a></h3><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">npm</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> @huggingface/inference</span></span></code></pre></div><p>Get a token from <a href="https://huggingface.co/settings/tokens" target="_blank" rel="noreferrer">Hugging Face Settings</a>.</p><h3 id="text-generation-llama-3" tabindex="-1">Text Generation (Llama 3) <a class="header-anchor" href="#text-generation-llama-3" aria-label="Permalink to &quot;Text Generation (Llama 3)&quot;">​</a></h3><div class="language-typescript vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">typescript</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> { HfInference } </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;@huggingface/inference&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> hf</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> new</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> HfInference</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(process.env.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">HF_TOKEN</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">);</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">async</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> function</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> main</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">() {</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">  const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> stream</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> hf.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">textGenerationStream</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">({</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    model: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;meta-llama/Meta-Llama-3-8B-Instruct&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    inputs: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Explain why the sky is blue.&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    parameters: {</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">      max_new_tokens: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">200</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">      temperature: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.7</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    },</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  });</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">  for</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> await</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> chunk</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> of</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> stream) {</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    process.stdout.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">write</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(chunk.token.text);</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  }</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}</span></span></code></pre></div><h3 id="embeddings-feature-extraction" tabindex="-1">Embeddings (Feature Extraction) <a class="header-anchor" href="#embeddings-feature-extraction" aria-label="Permalink to &quot;Embeddings (Feature Extraction)&quot;">​</a></h3><p>Generate vector embeddings for semantic search.</p><div class="language-typescript vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">typescript</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> output</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> await</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> hf.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">featureExtraction</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">({</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  model: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;sentence-transformers/all-MiniLM-L6-v2&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  inputs: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;That is a happy person&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">});</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// output is a Float32Array of numbers</span></span></code></pre></div><h2 id="free-vs-pro" tabindex="-1">Free vs. Pro <a class="header-anchor" href="#free-vs-pro" aria-label="Permalink to &quot;Free vs. Pro&quot;">​</a></h2><ul><li><strong>Free Tier</strong>: Rate limited, suitable for testing and development. Models may be &quot;cold&quot; (slow start).</li><li><strong>Pro Account ($9/mo)</strong>: Higher rate limits, access to gated models.</li><li><strong>Inference Endpoints</strong>: Dedicated GPU instances (pay per hour) for production traffic.</li></ul><h2 id="local-ai-with-transformers-js" tabindex="-1">Local AI with Transformers.js <a class="header-anchor" href="#local-ai-with-transformers-js" aria-label="Permalink to &quot;Local AI with Transformers.js&quot;">​</a></h2><p>You can run models <strong>directly in the user&#39;s browser</strong> with no server costs.</p><p><strong>See detailed guide</strong>: <a href="./../frontend-ml/transformersjs.html">Frontend ML (Transformers.js)</a></p><h3 id="basic-browser-example" tabindex="-1">Basic Browser Example <a class="header-anchor" href="#basic-browser-example" aria-label="Permalink to &quot;Basic Browser Example&quot;">​</a></h3><div class="language-javascript vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">javascript</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> { pipeline } </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;@xenova/transformers&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// Downloads the model to the browser cache (only once)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> classifier</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> await</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> pipeline</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;sentiment-analysis&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">);</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> result</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> await</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> classifier</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;I love using open source tools!&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">);</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// [{ label: &#39;POSITIVE&#39;, score: 0.99 }]</span></span></code></pre></div><h2 id="when-to-use-hugging-face" tabindex="-1">When to use Hugging Face? <a class="header-anchor" href="#when-to-use-hugging-face" aria-label="Permalink to &quot;When to use Hugging Face?&quot;">​</a></h2><ol><li><strong>Cost</strong>: Many models are free to test.</li><li><strong>Privacy</strong>: Use Inference Endpoints for private deployments or run locally.</li><li><strong>Specialized Tasks</strong>: Find models for specific niches (biology, legal, code) that general LLMs might struggle with.</li></ol><h2 id="next-steps" tabindex="-1">Next Steps <a class="header-anchor" href="#next-steps" aria-label="Permalink to &quot;Next Steps&quot;">​</a></h2><ul><li>Learn about <a href="./streaming.html"><strong>Streaming</strong></a> patterns.</li><li>Explore <a href="./../frontend-ml/"><strong>Frontend ML</strong></a> for browser-based AI.</li></ul>`,23)])])}const y=i(t,[["render",l]]);export{u as __pageData,y as default};
