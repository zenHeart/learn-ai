import{_ as e}from"./chunks/plugin-vue_export-helper.DlAUqK2U.js";import{c as t,o as s,ad as a}from"./chunks/mermaid.Cb40Nz1P.js";import"./chunks/cytoscape.C2CwDKBM.js";import"./chunks/dayjs.C32PoDnw.js";const m=JSON.parse('{"title":"Cost Optimization","description":"","frontmatter":{},"headers":[],"relativePath":"tech/engineering/cost-optimization.md","filePath":"tech/engineering/cost-optimization.md"}'),o={name:"tech/engineering/cost-optimization.md"};function n(r,i,h,l,p,d){return s(),t("div",null,[...i[0]||(i[0]=[a(`<h1 id="cost-optimization" tabindex="-1">Cost Optimization <a class="header-anchor" href="#cost-optimization" aria-label="Permalink to &quot;Cost Optimization&quot;">​</a></h1><p>AI is expensive. A single GPT-4 request can cost $0.03. If you have 10,000 users, that&#39;s $300/day.</p><h2 id="strategies" tabindex="-1">Strategies <a class="header-anchor" href="#strategies" aria-label="Permalink to &quot;Strategies&quot;">​</a></h2><h3 id="_1-model-routing-the-80-20-rule" tabindex="-1">1. Model Routing (The 80/20 Rule) <a class="header-anchor" href="#_1-model-routing-the-80-20-rule" aria-label="Permalink to &quot;1. Model Routing (The 80/20 Rule)&quot;">​</a></h3><p>80% of user queries are simple (&quot;Hi&quot;, &quot;Thanks&quot;, &quot;Summarize this short text&quot;). <strong>Don&#39;t use GPT-4 for everything.</strong></p><div class="language-typescript vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">typescript</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> isComplex</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> await</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> classifier.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">classify</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(prompt); </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// Cheap BERT model</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> isComplex </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">?</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;gpt-4o&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> :</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;gpt-4o-mini&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">;</span></span></code></pre></div><p><strong>Savings</strong>: 20x cheaper.</p><h3 id="_2-semantic-caching" tabindex="-1">2. Semantic Caching <a class="header-anchor" href="#_2-semantic-caching" aria-label="Permalink to &quot;2. Semantic Caching&quot;">​</a></h3><p>If User A asks &quot;Who is the President?&quot;, and User B asks &quot;Who is the current President?&quot;, they should get the same cached answer. Use <strong>Redis</strong> or a specialized cache (GPTCache) to store <code>(embedding(prompt), response)</code>.</p><p><strong>Savings</strong>: 100% (Free).</p><h3 id="_3-prompt-compression" tabindex="-1">3. Prompt Compression <a class="header-anchor" href="#_3-prompt-compression" aria-label="Permalink to &quot;3. Prompt Compression&quot;">​</a></h3><p>Shorter prompts = Lower cost.</p><ul><li>Remove polite phrases (&quot;Please&quot;, &quot;Thank you&quot;).</li><li>Use specialized syntax instead of verbose English.</li></ul><h3 id="_4-self-hosting-for-high-volume" tabindex="-1">4. Self-Hosting (for high volume) <a class="header-anchor" href="#_4-self-hosting-for-high-volume" aria-label="Permalink to &quot;4. Self-Hosting (for high volume)&quot;">​</a></h3><p>If you spend &gt; $5k/month, consider hosting Llama 3 on your own GPU server (AWS EC2 / RunPod).</p><h2 id="budgeting" tabindex="-1">Budgeting <a class="header-anchor" href="#budgeting" aria-label="Permalink to &quot;Budgeting&quot;">​</a></h2><p><strong>Formula</strong>: <code>Cost = (Input Tokens * Price_In) + (Output Tokens * Price_Out)</code></p><p><strong>Rule of Thumb</strong>:</p><ul><li>1,000 tokens ≈ 750 words.</li><li>Output is usually 3x more expensive than Input.</li><li>RAG apps have Huge Inputs (Context) and Small Outputs.</li></ul><h2 id="alerts" tabindex="-1">Alerts <a class="header-anchor" href="#alerts" aria-label="Permalink to &quot;Alerts&quot;">​</a></h2><p>Set a <strong>Hard Limit</strong> in OpenAI. If you don&#39;t, a <code>while(true)</code> loop in your code could cost you $10,000 overnight.</p>`,21)])])}const f=e(o,[["render",n]]);export{m as __pageData,f as default};
