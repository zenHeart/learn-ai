import{_ as a}from"./chunks/plugin-vue_export-helper.DlAUqK2U.js";import{c as t,o as n,ad as o}from"./chunks/mermaid.Cb40Nz1P.js";import"./chunks/cytoscape.C2CwDKBM.js";import"./chunks/dayjs.C32PoDnw.js";const m=JSON.parse('{"title":"PEFT (Parameter-Efficient Fine-Tuning)","description":"","frontmatter":{},"headers":[],"relativePath":"tech/training/PEFT.md","filePath":"tech/training/PEFT.md"}'),r={name:"tech/training/PEFT.md"};function i(s,e,l,d,u,g){return n(),t("div",null,[...e[0]||(e[0]=[o('<h1 id="peft-parameter-efficient-fine-tuning" tabindex="-1">PEFT (Parameter-Efficient Fine-Tuning) <a class="header-anchor" href="#peft-parameter-efficient-fine-tuning" aria-label="Permalink to &quot;PEFT (Parameter-Efficient Fine-Tuning)&quot;">​</a></h1><h2 id="what-is-peft" tabindex="-1">What is PEFT? <a class="header-anchor" href="#what-is-peft" aria-label="Permalink to &quot;What is PEFT?&quot;">​</a></h2><p><strong>Parameter-Efficient Fine-Tuning (PEFT)</strong> is a set of techniques to fine-tune Large Language Models (LLMs) without retraining all parameters. Instead of updating billions of weights, PEFT updates only a small fraction (often &lt; 1%) of added parameters.</p><p><strong>The Core Idea</strong>: Freeze the massive pre-trained model and only train small adapter layers.</p><p><strong>Benefits</strong>:</p><ul><li><strong>Lower Hardware Cost</strong>: Can run on consumer GPUs (e.g., a single RTX 4090 instead of an A100 cluster).</li><li><strong>Storage Efficiency</strong>: &quot;Adapters&quot; are small files (MBs) vs full models (GBs).</li><li><strong>Multi-Tenancy</strong>: You can serve one base model and swap small adapters for different users/tasks on the fly.</li></ul><h2 id="lora-low-rank-adaptation" tabindex="-1">LoRA (Low-Rank Adaptation) <a class="header-anchor" href="#lora-low-rank-adaptation" aria-label="Permalink to &quot;LoRA (Low-Rank Adaptation)&quot;">​</a></h2><p><strong>LoRA</strong> is the most popular PEFT technique.</p><ul><li><strong>How it works</strong>: It injects small &quot;rank decomposition matrices&quot; into the model and trains only those.</li><li><strong>Analogy</strong>: Imagine editing a book. Instead of rewriting the whole book (Full Fine-Tuning), you just write your edits on sticky notes and stick them on the pages (LoRA). When reading, you read the original page + the sticky note.</li></ul><h2 id="when-companies-use-peft" tabindex="-1">When Companies Use PEFT <a class="header-anchor" href="#when-companies-use-peft" aria-label="Permalink to &quot;When Companies Use PEFT&quot;">​</a></h2><ol><li><strong>Cost-Effective Customization</strong>: When they need a custom model for a specific task (e.g., &quot;SQL Generator&quot;) but can&#39;t afford to train a full 70B parameter model.</li><li><strong>Privacy/On-Premise</strong>: Running fine-tuned open-source models (Llama 3, Mistral) on local hardware.</li><li><strong>Personalized AI</strong>: Creating thousands of user-specific models (e.g., one style adapter per user) that share one base model.</li></ol><div class="tip custom-block"><p class="custom-block-title">Frontend Relevance</p><p><strong>Running Local LLMs</strong></p><p>If you use tools like <strong>Ollama</strong> or <strong>LM Studio</strong>, you are often downloading &quot;quantized&quot; models or applying LoRA adapters. Understanding PEFT helps you understand why you can run powerful AI on your MacBook.</p></div><h2 id="further-reading" tabindex="-1">Further Reading <a class="header-anchor" href="#further-reading" aria-label="Permalink to &quot;Further Reading&quot;">​</a></h2><ul><li><a href="https://arxiv.org/abs/2106.09685" target="_blank" rel="noreferrer">LoRA: Low-Rank Adaptation of Large Language Models (Paper)</a></li><li><a href="https://github.com/huggingface/peft" target="_blank" rel="noreferrer">Hugging Face PEFT Library</a></li></ul>',14)])])}const P=a(r,[["render",i]]);export{m as __pageData,P as default};
