import{_ as a}from"./chunks/plugin-vue_export-helper.DlAUqK2U.js";import{c as r,o,ad as e}from"./chunks/mermaid.Cb40Nz1P.js";import"./chunks/cytoscape.C2CwDKBM.js";import"./chunks/dayjs.C32PoDnw.js";const P=JSON.parse('{"title":"PEFT (参数高效微调)","description":"","frontmatter":{},"headers":[],"relativePath":"zh/tech/training/PEFT.md","filePath":"zh/tech/training/PEFT.md"}'),n={name:"zh/tech/training/PEFT.md"};function i(l,t,s,g,p,h){return o(),r("div",null,[...t[0]||(t[0]=[e('<h1 id="peft-参数高效微调" tabindex="-1">PEFT (参数高效微调) <a class="header-anchor" href="#peft-参数高效微调" aria-label="Permalink to &quot;PEFT (参数高效微调)&quot;">​</a></h1><h2 id="什么是-peft" tabindex="-1">什么是 PEFT？ <a class="header-anchor" href="#什么是-peft" aria-label="Permalink to &quot;什么是 PEFT？&quot;">​</a></h2><p><strong>参数高效微调 (Parameter-Efficient Fine-Tuning, PEFT)</strong> 是一组技术，用于在不重新训练所有参数的情况下微调大语言模型 (LLM)。PEFT 不是更新数十亿个权重，而是仅更新一小部分（通常 &lt; 1%）添加的参数。</p><p><strong>核心思想</strong>: 冻结大规模预训练模型，仅训练小的适配器层 (adapter layers)。</p><p><strong>好处</strong>:</p><ul><li><strong>更低的硬件成本</strong>: 可以在消费级 GPU 上运行（例如，单个 RTX 4090 而不是 A100 集群）。</li><li><strong>存储效率</strong>: &quot;适配器&quot; 是小文件 (MBs) vs 完整模型 (GBs)。</li><li><strong>多租户</strong>: 你可以服务一个基础模型，并即时为不同的用户/任务交换小的适配器。</li></ul><h2 id="lora-低秩适应" tabindex="-1">LoRA (低秩适应) <a class="header-anchor" href="#lora-低秩适应" aria-label="Permalink to &quot;LoRA (低秩适应)&quot;">​</a></h2><p><strong>LoRA (Low-Rank Adaptation)</strong> 是最流行的 PEFT 技术。</p><ul><li><strong>工作原理</strong>: 它将小的“低秩分解矩阵”注入模型中，并仅训练这些矩阵。</li><li><strong>类比</strong>: 想象编辑一本书。不是重写整本书（全量微调），你只是在便利贴上写下你的编辑，并将它们贴在页面上（LoRA）。阅读时，你阅读原始页面 + 便利贴。</li></ul><h2 id="公司何时使用-peft" tabindex="-1">公司何时使用 PEFT <a class="header-anchor" href="#公司何时使用-peft" aria-label="Permalink to &quot;公司何时使用 PEFT&quot;">​</a></h2><ol><li><strong>具有成本效益的定制</strong>: 当他们需要针对特定任务（例如，“SQL 生成器”）的自定义模型，但无法承担训练完整 70B 参数模型的费用时。</li><li><strong>隐私/本地部署</strong>: 在本地硬件上运行微调过的开源模型（Llama 3, Mistral）。</li><li><strong>个性化 AI</strong>: 创建数千个特定于用户的模型（例如，每个用户一个风格适配器），共享同一个基础模型。</li></ol><div class="tip custom-block"><p class="custom-block-title">前端相关性</p><p><strong>运行本地 LLM</strong></p><p>如果你使用 <strong>Ollama</strong> 或 <strong>LM Studio</strong> 等工具，你通常是在下载“量化”模型或应用 LoRA 适配器。了解 PEFT 有助于你理解为什么你可以在 MacBook 上运行强大的 AI。</p></div><h2 id="延伸阅读" tabindex="-1">延伸阅读 <a class="header-anchor" href="#延伸阅读" aria-label="Permalink to &quot;延伸阅读&quot;">​</a></h2><ul><li><a href="https://arxiv.org/abs/2106.09685" target="_blank" rel="noreferrer">LoRA: Low-Rank Adaptation of Large Language Models (Paper)</a></li><li><a href="https://github.com/huggingface/peft" target="_blank" rel="noreferrer">Hugging Face PEFT Library</a></li></ul>',14)])])}const m=a(n,[["render",i]]);export{P as __pageData,m as default};
