import{_ as i}from"./chunks/plugin-vue_export-helper.DlAUqK2U.js";import{c as s,o as e,ad as t}from"./chunks/mermaid.Cb40Nz1P.js";import"./chunks/cytoscape.C2CwDKBM.js";import"./chunks/dayjs.C32PoDnw.js";const y=JSON.parse('{"title":"Caching Strategies for AI","description":"","frontmatter":{},"headers":[],"relativePath":"deployment/caching.md","filePath":"deployment/caching.md"}'),n={name:"deployment/caching.md"};function h(l,a,r,o,p,c){return e(),s("div",null,[...a[0]||(a[0]=[t(`<h1 id="caching-strategies-for-ai" tabindex="-1">Caching Strategies for AI <a class="header-anchor" href="#caching-strategies-for-ai" aria-label="Permalink to &quot;Caching Strategies for AI&quot;">​</a></h1><p>LLM requests are <strong>slow</strong> and <strong>expensive</strong>. Caching is the best way to fix both.</p><h2 id="_1-standard-api-caching-vercel-data-cache" tabindex="-1">1. Standard API Caching (Vercel Data Cache) <a class="header-anchor" href="#_1-standard-api-caching-vercel-data-cache" aria-label="Permalink to &quot;1. Standard API Caching (Vercel Data Cache)&quot;">​</a></h2><p>If the prompt is static (e.g., &quot;Generate the daily horoscope&quot;), cache the response.</p><div class="language-typescript vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">typescript</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// Next.js App Router</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">export</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> revalidate</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 3600</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">; </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// Cache for 1 hour</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">export</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> async</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> function</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> GET</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">() {</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">  const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> completion</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> await</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> openai.chat.completions.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">create</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">({ </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">...</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> });</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">  return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Response.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">json</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(completion);</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}</span></span></code></pre></div><h2 id="_2-semantic-caching-the-holy-grail" tabindex="-1">2. Semantic Caching (The Holy Grail) <a class="header-anchor" href="#_2-semantic-caching-the-holy-grail" aria-label="Permalink to &quot;2. Semantic Caching (The Holy Grail)&quot;">​</a></h2><p>Users rarely type the exact same thing twice.</p><ul><li>User A: &quot;Who is Elon Musk?&quot;</li><li>User B: &quot;Tell me about Elon Musk&quot;</li></ul><p>Standard cache misses. <strong>Semantic Cache</strong> hits.</p><p><strong>How it works</strong>:</p><ol><li>Embed the incoming prompt.</li><li>Search your Vector DB (Redis/Pinecone) for similar past prompts (threshold &gt; 0.95).</li><li>If found, return the stored answer.</li></ol><p><strong>Libraries</strong>:</p><ul><li><strong>GPTCache</strong>: Python library.</li><li><strong>Upstash Semantic Cache</strong>: Serverless solution.</li></ul><h2 id="_3-edge-caching-cdn" tabindex="-1">3. Edge Caching (CDN) <a class="header-anchor" href="#_3-edge-caching-cdn" aria-label="Permalink to &quot;3. Edge Caching (CDN)&quot;">​</a></h2><p>For assets generated by AI (Images, Audio), always cache them on the Edge (CDN). Don&#39;t serve generated images from your database; upload them to S3/R2 and serve via Cloudflare/Vercel Edge.</p><h2 id="cache-invalidation" tabindex="-1">Cache Invalidation <a class="header-anchor" href="#cache-invalidation" aria-label="Permalink to &quot;Cache Invalidation&quot;">​</a></h2><p>AI models change (updates from OpenAI).</p><ul><li><strong>Time-based</strong>: Expire cache every week.</li><li><strong>Model-based</strong>: Invalidate all cache when you upgrade from <code>gpt-3.5</code> to <code>gpt-4o</code>.</li></ul>`,18)])])}const m=i(n,[["render",h]]);export{y as __pageData,m as default};
