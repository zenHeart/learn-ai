import{_ as e}from"./chunks/plugin-vue_export-helper.DlAUqK2U.js";import{c as s,o as a,ad as i}from"./chunks/mermaid.Cb40Nz1P.js";import"./chunks/cytoscape.C2CwDKBM.js";import"./chunks/dayjs.C32PoDnw.js";const u=JSON.parse('{"title":"Observability","description":"","frontmatter":{},"headers":[],"relativePath":"tech/engineering/observability.md","filePath":"tech/engineering/observability.md"}'),n={name:"tech/engineering/observability.md"};function l(r,t,o,p,d,h){return a(),s("div",null,[...t[0]||(t[0]=[i(`<h1 id="observability" tabindex="-1">Observability <a class="header-anchor" href="#observability" aria-label="Permalink to &quot;Observability&quot;">​</a></h1><p>In traditional software, you log &quot;Request / Response&quot;. In AI software, you must log <strong>Prompts, Completions, Tokens, and Latency</strong>.</p><h2 id="key-metrics-to-track" tabindex="-1">Key Metrics to Track <a class="header-anchor" href="#key-metrics-to-track" aria-label="Permalink to &quot;Key Metrics to Track&quot;">​</a></h2><table tabindex="0"><thead><tr><th style="text-align:left;">Metric</th><th style="text-align:left;">Definition</th><th style="text-align:left;">Goal</th></tr></thead><tbody><tr><td style="text-align:left;"><strong>TTFT</strong></td><td style="text-align:left;">Time To First Token</td><td style="text-align:left;">&lt; 1000ms</td></tr><tr><td style="text-align:left;"><strong>Total Latency</strong></td><td style="text-align:left;">Time to full completion</td><td style="text-align:left;">&lt; 5s (context dependent)</td></tr><tr><td style="text-align:left;"><strong>Tokens</strong></td><td style="text-align:left;">Input + Output length</td><td style="text-align:left;">Minimize (Cost saving)</td></tr><tr><td style="text-align:left;"><strong>Cost</strong></td><td style="text-align:left;">$$$ per request</td><td style="text-align:left;">Track budget</td></tr><tr><td style="text-align:left;"><strong>Errors</strong></td><td style="text-align:left;">429s, 500s</td><td style="text-align:left;">&lt; 1%</td></tr></tbody></table><h2 id="tracing" tabindex="-1">Tracing <a class="header-anchor" href="#tracing" aria-label="Permalink to &quot;Tracing&quot;">​</a></h2><p>AI Apps are complex chains (RAG: Retriever -&gt; Reranker -&gt; Generator). <strong>Tracing</strong> visualizes the entire waterfall of a request.</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>[Request] &quot;Who is CEO of Apple?&quot; (2s)</span></span>
<span class="line"><span>  ├── [Retrieve] Pinecone (0.5s)</span></span>
<span class="line"><span>  ├── [Rerank] Cohere (0.2s)</span></span>
<span class="line"><span>  └── [Generate] GPT-4o (1.3s)</span></span></code></pre></div><h2 id="tools" tabindex="-1">Tools <a class="header-anchor" href="#tools" aria-label="Permalink to &quot;Tools&quot;">​</a></h2><p>Don&#39;t build this yourself. Use a specialized proxy.</p><ol><li><strong>Helicone</strong>: Open-source proxy. Just change the <code>baseURL</code> in OpenAI SDK.<div class="language-typescript vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">typescript</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> openai</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> new</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> OpenAI</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">({</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  baseURL: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;https://oai.hconeai.com/v1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  defaultHeaders: { </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Helicone-Auth&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Bearer HELICONE_KEY&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> }</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">});</span></span></code></pre></div></li><li><strong>Langfuse</strong>: Open-source tracing and metrics.</li><li><strong>Sentry</strong>: Good for error tracking, but less detailed for token costs.</li></ol><h2 id="privacy-redaction" tabindex="-1">Privacy Redaction <a class="header-anchor" href="#privacy-redaction" aria-label="Permalink to &quot;Privacy Redaction&quot;">​</a></h2><p><strong>Warning</strong>: Never log PII (Personally Identifiable Information) like emails or passwords to these 3rd party tools. Implement a redaction layer <em>before</em> logging.</p>`,12)])])}const b=e(n,[["render",l]]);export{u as __pageData,b as default};
