import{_ as o}from"./chunks/plugin-vue_export-helper.DlAUqK2U.js";import{C as r,c as s,o as t,ad as a,b as l,w as n,a as d,G as g,ae as h}from"./chunks/mermaid.Cb40Nz1P.js";import"./chunks/cytoscape.C2CwDKBM.js";import"./chunks/dayjs.C32PoDnw.js";const P=JSON.parse('{"title":"Tech Stack","description":"","frontmatter":{},"headers":[],"relativePath":"tech/index.md","filePath":"tech/index.md"}'),p={name:"tech/index.md"};function c(m,e,u,k,f,b){const i=r("Mermaid");return t(),s("div",null,[e[1]||(e[1]=a('<h1 id="tech-stack" tabindex="-1">Tech Stack <a class="header-anchor" href="#tech-stack" aria-label="Permalink to &quot;Tech Stack&quot;">​</a></h1><p>This section explores the core AI technologies and patterns you&#39;ll use to build intelligent applications. Each technology solves specific problems - understanding WHEN to use WHAT is critical.</p><h2 id="the-three-pillars-of-ai-application-development" tabindex="-1">The Three Pillars of AI Application Development <a class="header-anchor" href="#the-three-pillars-of-ai-application-development" aria-label="Permalink to &quot;The Three Pillars of AI Application Development&quot;">​</a></h2><h3 id="_1-llm-large-language-models-the-foundation" tabindex="-1">1. <strong>LLM (Large Language Models)</strong> - The Foundation <a class="header-anchor" href="#_1-llm-large-language-models-the-foundation" aria-label="Permalink to &quot;1. **LLM (Large Language Models)** - The Foundation&quot;">​</a></h3><p><strong>What</strong>: Pre-trained models that understand and generate text.</p><p><strong>When to use</strong>: All AI applications start here.</p><p><strong>Learn more</strong>: <a href="/learn-ai/tech/fundamentals/LLM.html">LLM Guide</a></p><h3 id="_2-prompt-engineering-the-interface" tabindex="-1">2. <strong>Prompt Engineering</strong> - The Interface <a class="header-anchor" href="#_2-prompt-engineering-the-interface" aria-label="Permalink to &quot;2. **Prompt Engineering** - The Interface&quot;">​</a></h3><p><strong>What</strong>: Techniques to communicate effectively with LLMs.</p><p><strong>When to use</strong>: Every interaction with an LLM requires good prompts.</p><p><strong>Learn more</strong>: <a href="/learn-ai/tech/prompt/">Prompt Engineering</a></p><h3 id="_3-context-management-the-memory" tabindex="-1">3. <strong>Context Management</strong> - The Memory <a class="header-anchor" href="#_3-context-management-the-memory" aria-label="Permalink to &quot;3. **Context Management** - The Memory&quot;">​</a></h3><p><strong>What</strong>: Managing what information the LLM can see and remember.</p><p><strong>When to use</strong>: Chat applications, document Q&amp;A, any multi-turn interaction.</p><p><strong>Learn more</strong>: <a href="/learn-ai/tech/fundamentals/context.html">Context Guide</a></p><h2 id="core-technologies-comparison" tabindex="-1">Core Technologies Comparison <a class="header-anchor" href="#core-technologies-comparison" aria-label="Permalink to &quot;Core Technologies Comparison&quot;">​</a></h2><h3 id="fine-tuning-sft-vs-rag" tabindex="-1">Fine-Tuning (SFT) vs RAG <a class="header-anchor" href="#fine-tuning-sft-vs-rag" aria-label="Permalink to &quot;Fine-Tuning (SFT) vs RAG&quot;">​</a></h3><table tabindex="0"><thead><tr><th>Aspect</th><th>Fine-Tuning (SFT)</th><th>RAG (Retrieval-Augmented Generation)</th></tr></thead><tbody><tr><td><strong>Core Principle</strong></td><td>Adjust model parameters for specific tasks</td><td>Retrieve external knowledge to enhance generation</td></tr><tr><td><strong>Cost</strong></td><td><strong>High</strong> - Requires GPUs, labeled data, retraining</td><td><strong>Low</strong> - Uses external knowledge base, inference only</td></tr><tr><td><strong>Response Time</strong></td><td><strong>Fast</strong> - Knowledge built into model</td><td><strong>Slower</strong> - Requires retrieval step before generation</td></tr><tr><td><strong>Best For</strong></td><td>Domain-specific terminology, complex reasoning</td><td>High accuracy requirements, frequently updated knowledge</td></tr><tr><td><strong>Maintenance</strong></td><td><strong>Hard</strong> - Requires retraining for updates</td><td><strong>Easy</strong> - Update knowledge base anytime</td></tr><tr><td><strong>Control</strong></td><td>Black box, model behavior fixed after training</td><td>Transparent, controllable through knowledge base</td></tr><tr><td><strong>Setup Complexity</strong></td><td>Very high (days/weeks)</td><td>Medium (hours/days)</td></tr><tr><td><strong>When to Use</strong></td><td>Specialized domains, stable knowledge</td><td>Dynamic data, explainable AI, cost-sensitive</td></tr></tbody></table><p><strong>Frontend Engineer Recommendation</strong>: Start with RAG. Only consider fine-tuning if you have:</p><ul><li>Highly specialized domain (medical, legal, etc.)</li><li>Budget for GPU training</li><li>Stable knowledge that rarely changes</li></ul><h2 id="technology-decision-tree" tabindex="-1">Technology Decision Tree <a class="header-anchor" href="#technology-decision-tree" aria-label="Permalink to &quot;Technology Decision Tree&quot;">​</a></h2>',21)),(t(),l(h,null,{default:n(()=>[g(i,{id:"mermaid-179",class:"mermaid",graph:"graph%20TD%0A%20%20%20%20A%7BNeed%20Specialized%20Model%3F%7D%20--%3E%7CYes%7C%20B%7BTask%20Complexity%3F%7D%0A%20%20%20%20A%20--%3E%7CNo%7C%20C%7BNeed%20External%20Knowledge%3F%7D%0A%20%20%20%20B%20--%3E%7CSimple%7C%20D%5BSFT%20Only%5D%0A%20%20%20%20B%20--%3E%7CComplex%7C%20E%5BRAG%20%2B%20SFT%5D%0A%20%20%20%20C%20--%3E%7CNo%7C%20F%5BPrompt%20Engineering%20Only%5D%0A%20%20%20%20C%20--%3E%7CYes%7C%20G%5BRAG%5D%0A%0A%20%20%20%20style%20F%20fill%3A%2390EE90%0A%20%20%20%20style%20G%20fill%3A%2387CEEB%0A%20%20%20%20style%20D%20fill%3A%23FFB6C1%0A%20%20%20%20style%20E%20fill%3A%23DDA0DD%0A"})]),fallback:n(()=>[...e[0]||(e[0]=[d(" Loading... ",-1)])]),_:1})),e[2]||(e[2]=a(`<h3 id="decision-guide" tabindex="-1">Decision Guide <a class="header-anchor" href="#decision-guide" aria-label="Permalink to &quot;Decision Guide&quot;">​</a></h3><p><strong>Scenario 1: Building a chatbot for your product docs</strong></p><ul><li>✅ Use: RAG (frequently updated docs)</li><li>❌ Don&#39;t use: Fine-tuning (expensive, hard to update)</li></ul><p><strong>Scenario 2: Building a code completion tool</strong></p><ul><li>✅ Use: Prompt Engineering + Context Management</li><li>Maybe: RAG for project-specific patterns</li><li>❌ Don&#39;t use: Fine-tuning (unless you&#39;re GitHub Copilot)</li></ul><p><strong>Scenario 3: Medical diagnosis assistant</strong></p><ul><li>✅ Use: RAG + Fine-tuning</li><li>Why: Specialized terminology (SFT) + Latest research (RAG)</li></ul><p><strong>Scenario 4: Simple AI chat feature</strong></p><ul><li>✅ Use: Prompt Engineering only</li><li>Why: Most LLMs are already good at conversation</li></ul><h2 id="how-to-build-ai-applications-practical-pattern" tabindex="-1">How to Build AI Applications (Practical Pattern) <a class="header-anchor" href="#how-to-build-ai-applications-practical-pattern" aria-label="Permalink to &quot;How to Build AI Applications (Practical Pattern)&quot;">​</a></h2><h3 id="step-1-start-with-prompt-engineering" tabindex="-1">Step 1: Start with Prompt Engineering <a class="header-anchor" href="#step-1-start-with-prompt-engineering" aria-label="Permalink to &quot;Step 1: Start with Prompt Engineering&quot;">​</a></h3><p><strong>Goal</strong>: Make LLMs do what you want with good prompts.</p><p><strong>Techniques</strong>:</p><ul><li><strong>RTF</strong> (Role, Task, Format) - Structure your prompts</li><li><strong>COT</strong> (Chain of Thought) - Use logical reasoning</li><li><strong>FSP</strong> (Few-Shot Prompting) - Show examples</li><li><strong>RAG</strong> - Inject relevant knowledge inline</li></ul><p><strong>Learn more</strong>: <a href="/learn-ai/tech/prompt/">Prompt Engineering Guide</a></p><h3 id="step-2-add-rag-for-external-knowledge" tabindex="-1">Step 2: Add RAG for External Knowledge <a class="header-anchor" href="#step-2-add-rag-for-external-knowledge" aria-label="Permalink to &quot;Step 2: Add RAG for External Knowledge&quot;">​</a></h3><p><strong>Goal</strong>: Connect LLMs to your data (docs, databases, APIs).</p><p><strong>How it works</strong>:</p><ol><li>Store documents as vector embeddings</li><li>Retrieve relevant chunks based on user query</li><li>Inject chunks into LLM prompt</li><li>Get accurate, up-to-date answers</li></ol><p><strong>Learn more</strong>: <a href="/learn-ai/tech/patterns/RAG.html">RAG Guide</a></p><h3 id="step-3-use-interceptors-for-quality-control" tabindex="-1">Step 3: Use Interceptors for Quality Control <a class="header-anchor" href="#step-3-use-interceptors-for-quality-control" aria-label="Permalink to &quot;Step 3: Use Interceptors for Quality Control&quot;">​</a></h3><p><strong>Goal</strong>: Prevent LLM hallucinations and errors.</p><p><strong>Checks to implement</strong>:</p><ol><li><p><strong>Format validation</strong></p><div class="language-javascript vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">javascript</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">!</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">isValidJSON</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(llmOutput)) {</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">  throw</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> new</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> Error</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Invalid format, retry&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">);</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}</span></span></code></pre></div></li><li><p><strong>Business rule enforcement</strong></p><div class="language-javascript vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">javascript</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (llmOutput.price </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&lt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) {</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">  throw</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> new</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> Error</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Invalid price, regenerate&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">);</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}</span></span></code></pre></div></li><li><p><strong>Content safety</strong></p><div class="language-javascript vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">javascript</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">containsHarmfulContent</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(llmOutput)) {</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">  return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> fallbackResponse;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}</span></span></code></pre></div></li></ol><h3 id="step-4-implement-agent-patterns-advanced" tabindex="-1">Step 4: Implement Agent Patterns (Advanced) <a class="header-anchor" href="#step-4-implement-agent-patterns-advanced" aria-label="Permalink to &quot;Step 4: Implement Agent Patterns (Advanced)&quot;">​</a></h3><p><strong>Goal</strong>: Build autonomous AI systems that use tools and make decisions.</p><p><strong>Capabilities</strong>:</p><ul><li>Tool calling (MCP)</li><li>Multi-step reasoning</li><li>Error recovery</li><li>Self-correction</li></ul><p><strong>Learn more</strong>: <a href="/learn-ai/tech/patterns/agent/">Agent Guide</a>, <a href="/learn-ai/integration/protocols/mcp.html">MCP Protocol</a></p><h2 id="technology-stack-by-use-case" tabindex="-1">Technology Stack by Use Case <a class="header-anchor" href="#technology-stack-by-use-case" aria-label="Permalink to &quot;Technology Stack by Use Case&quot;">​</a></h2><h3 id="chat-application" tabindex="-1">Chat Application <a class="header-anchor" href="#chat-application" aria-label="Permalink to &quot;Chat Application&quot;">​</a></h3><ul><li><strong>LLM</strong>: GPT-4 or Claude</li><li><strong>Context Management</strong>: Sliding window or summarization</li><li><strong>Prompt Engineering</strong>: System prompts for personality</li><li><strong>Optional</strong>: RAG for knowledge base</li></ul><h3 id="code-assistant" tabindex="-1">Code Assistant <a class="header-anchor" href="#code-assistant" aria-label="Permalink to &quot;Code Assistant&quot;">​</a></h3><ul><li><strong>LLM</strong>: Claude 3.5 Sonnet (best for code)</li><li><strong>Context Management</strong>: Large context window (200k tokens)</li><li><strong>MCP</strong>: File system, git, documentation tools</li><li><strong>Prompt Engineering</strong>: Few-shot examples</li></ul><h3 id="document-q-a" tabindex="-1">Document Q&amp;A <a class="header-anchor" href="#document-q-a" aria-label="Permalink to &quot;Document Q&amp;A&quot;">​</a></h3><ul><li><strong>RAG</strong>: Vector database (Pinecone, Weaviate, pgvector)</li><li><strong>LLM</strong>: GPT-3.5 Turbo (cost-effective) or GPT-4 (accuracy)</li><li><strong>Embeddings</strong>: text-embedding-ada-002</li><li><strong>Context Management</strong>: Chunk-based retrieval</li></ul><h3 id="ai-workflow-agent" tabindex="-1">AI Workflow / Agent <a class="header-anchor" href="#ai-workflow-agent" aria-label="Permalink to &quot;AI Workflow / Agent&quot;">​</a></h3><ul><li><strong>MCP</strong>: Tool calling protocol</li><li><strong>Agent</strong>: ReAct or Plan-and-Execute pattern</li><li><strong>LLM</strong>: GPT-4 or Claude 3.5 (tool use capabilities)</li><li><strong>Prompt Engineering</strong>: Structured outputs, function calling</li></ul><h2 id="recommended-tools-platforms" tabindex="-1">Recommended Tools &amp; Platforms <a class="header-anchor" href="#recommended-tools-platforms" aria-label="Permalink to &quot;Recommended Tools &amp; Platforms&quot;">​</a></h2><h3 id="development-frameworks" tabindex="-1">Development Frameworks <a class="header-anchor" href="#development-frameworks" aria-label="Permalink to &quot;Development Frameworks&quot;">​</a></h3><ol><li><strong>LangChain</strong> - Python/JS framework for LLM apps</li><li><strong>Vercel AI SDK</strong> - React-first AI framework</li><li><strong>LlamaIndex</strong> - RAG and data framework</li><li><strong>Anthropic SDK</strong> - Claude integration</li></ol><h3 id="vector-databases-rag" tabindex="-1">Vector Databases (RAG) <a class="header-anchor" href="#vector-databases-rag" aria-label="Permalink to &quot;Vector Databases (RAG)&quot;">​</a></h3><ol><li><strong>Pinecone</strong> - Managed vector DB</li><li><strong>Weaviate</strong> - Open-source vector search</li><li><strong>pgvector</strong> - Postgres extension</li><li><strong>Chroma</strong> - Lightweight, embeddable</li></ol><h3 id="low-code-platforms" tabindex="-1">Low-Code Platforms <a class="header-anchor" href="#low-code-platforms" aria-label="Permalink to &quot;Low-Code Platforms&quot;">​</a></h3><ol><li><strong>Dify</strong> - Open-source LLM app platform</li><li><strong>Flowise</strong> - Visual LLM workflow builder</li><li><strong>LangFlow</strong> - Drag-and-drop LLM chains</li></ol><h3 id="local-development" tabindex="-1">Local Development <a class="header-anchor" href="#local-development" aria-label="Permalink to &quot;Local Development&quot;">​</a></h3><ol><li><strong>Ollama</strong> - Run LLMs locally</li><li><strong>LocalAI</strong> - Self-hosted AI API</li><li><strong>LM Studio</strong> - GUI for local models</li></ol><h2 id="learning-path-recommendation" tabindex="-1">Learning Path Recommendation <a class="header-anchor" href="#learning-path-recommendation" aria-label="Permalink to &quot;Learning Path Recommendation&quot;">​</a></h2><p><strong>Week 1-2</strong>: Foundations</p><ul><li>Master <a href="/learn-ai/tech/fundamentals/LLM.html">LLM basics</a></li><li>Learn <a href="/learn-ai/tech/prompt/">Prompt Engineering</a></li><li>Understand <a href="/learn-ai/tech/fundamentals/context.html">Context Management</a></li></ul><p><strong>Week 3-4</strong>: Building Features</p><ul><li>Implement <a href="/learn-ai/tech/patterns/RAG.html">RAG</a> for knowledge retrieval</li><li>Explore <a href="/learn-ai/integration/protocols/mcp.html">MCP</a> for tool integration</li><li>Build simple <a href="/learn-ai/tech/patterns/agent/">Agents</a></li></ul><p><strong>Week 5+</strong>: Advanced Patterns</p><ul><li>Fine-tuning (<a href="/learn-ai/tech/training/SFT.html">SFT</a>) for specialized needs</li><li>Production optimization</li><li>Multi-agent workflows</li></ul><h2 id="common-pitfalls-to-avoid" tabindex="-1">Common Pitfalls to Avoid <a class="header-anchor" href="#common-pitfalls-to-avoid" aria-label="Permalink to &quot;Common Pitfalls to Avoid&quot;">​</a></h2><ol><li><strong>Over-engineering</strong>: Don&#39;t use RAG if simple prompts work</li><li><strong>Wrong model choice</strong>: Use cheaper models for simple tasks</li><li><strong>Ignoring context limits</strong>: Always manage token usage</li><li><strong>No error handling</strong>: LLMs fail - have fallbacks</li><li><strong>Premature fine-tuning</strong>: Try RAG and prompt engineering first</li></ol><h2 id="next-steps" tabindex="-1">Next Steps <a class="header-anchor" href="#next-steps" aria-label="Permalink to &quot;Next Steps&quot;">​</a></h2><p>Choose your learning path based on your immediate needs:</p><ul><li><strong>Need to understand basics?</strong> → Start with <a href="/learn-ai/tech/fundamentals/LLM.html">LLM</a></li><li><strong>Building a chat feature?</strong> → Check <a href="/learn-ai/tech/prompt/">Prompt Engineering</a></li><li><strong>Adding external knowledge?</strong> → Learn <a href="/learn-ai/tech/patterns/RAG.html">RAG</a></li><li><strong>Building autonomous AI?</strong> → Explore <a href="/learn-ai/tech/patterns/agent/">Agent</a> and <a href="/learn-ai/integration/protocols/mcp.html">MCP</a></li><li><strong>Managing costs/performance?</strong> → Read <a href="/learn-ai/tech/fundamentals/context.html">Context Management</a></li><li><strong>Need specialized model?</strong> → See <a href="/learn-ai/tech/training/SFT.html">SFT</a></li></ul><p>The technologies in this section are your building blocks. Master them to create powerful, production-ready AI applications.</p>`,60))])}const x=o(p,[["render",c]]);export{P as __pageData,x as default};
