import{_ as t}from"./chunks/plugin-vue_export-helper.DlAUqK2U.js";import{c as a,o as r,ad as n}from"./chunks/mermaid.Cb40Nz1P.js";import"./chunks/cytoscape.C2CwDKBM.js";import"./chunks/dayjs.C32PoDnw.js";const f=JSON.parse('{"title":"RLHF (Reinforcement Learning from Human Feedback)","description":"","frontmatter":{},"headers":[],"relativePath":"tech/training/RLHF.md","filePath":"tech/training/RLHF.md"}'),o={name:"tech/training/RLHF.md"};function i(s,e,l,h,u,d){return r(),a("div",null,[...e[0]||(e[0]=[n('<h1 id="rlhf-reinforcement-learning-from-human-feedback" tabindex="-1">RLHF (Reinforcement Learning from Human Feedback) <a class="header-anchor" href="#rlhf-reinforcement-learning-from-human-feedback" aria-label="Permalink to &quot;RLHF (Reinforcement Learning from Human Feedback)&quot;">​</a></h1><h2 id="what-is-rlhf" tabindex="-1">What is RLHF? <a class="header-anchor" href="#what-is-rlhf" aria-label="Permalink to &quot;What is RLHF?&quot;">​</a></h2><p><strong>Reinforcement Learning from Human Feedback (RLHF)</strong> is a training technique used to align Language Models with human values and preferences. It&#39;s the &quot;secret sauce&quot; that turned raw GPT-3 into the helpful ChatGPT assistant we use today.</p><p><strong>The Core Idea</strong>: Train a reward model based on human rankings of AI outputs, then use that reward model to fine-tune the LLM to generate &quot;better&quot; answers.</p><p><strong>Three Steps of RLHF</strong>:</p><ol><li><strong>Supervised Fine-Tuning (SFT)</strong>: Train model on human demonstrations.</li><li><strong>Reward Modeling</strong>: Humans rank multiple model outputs; train a &quot;Reward Model&quot; to predict these rankings.</li><li><strong>Reinforcement Learning (PPO)</strong>: Optimize the LLM to maximize the score from the Reward Model.</li></ol><div class="info custom-block"><p class="custom-block-title">Frontend Relevance</p><p><strong>You Will Not Implement This</strong></p><p>RLHF is extremely expensive and complex. It requires massive datasets of human feedback and significant GPU resources. As a frontend engineer, you only need to know that <strong>this is why models refuse to answer harmful questions</strong> or try to be &quot;helpful, honest, and harmless.&quot;</p></div><h2 id="used-by" tabindex="-1">Used By <a class="header-anchor" href="#used-by" aria-label="Permalink to &quot;Used By&quot;">​</a></h2><p>Virtually all top-tier Chat models use RLHF or similar alignment techniques (like RLAIF - AI Feedback):</p><ul><li><strong>OpenAI</strong>: ChatGPT (GPT-3.5, GPT-4)</li><li><strong>Anthropic</strong>: Claude (Constitutional AI, a variation of RLHF)</li><li><strong>Meta</strong>: Llama 2 &amp; 3 (Llama-2-chat)</li><li><strong>Google</strong>: Gemini</li></ul><h2 id="why-it-matters-for-application-development" tabindex="-1">Why It Matters for Application Development <a class="header-anchor" href="#why-it-matters-for-application-development" aria-label="Permalink to &quot;Why It Matters for Application Development&quot;">​</a></h2><p>Understanding RLHF helps explain certain model behaviors:</p><ol><li><strong>Refusals</strong>: Models might refuse benign requests because of over-optimization on safety data during RLHF.</li><li><strong>Verbosity</strong>: Models often become &quot;chatty&quot; or hedge their answers (&quot;As an AI language model...&quot;) due to RLHF patterns.</li><li><strong>Style</strong>: The &quot;personality&quot; of the model is largely shaped during this stage.</li></ol><h2 id="further-reading" tabindex="-1">Further Reading <a class="header-anchor" href="#further-reading" aria-label="Permalink to &quot;Further Reading&quot;">​</a></h2><ul><li><a href="https://openai.com/research/learning-from-human-preferences" target="_blank" rel="noreferrer">Deep reinforcement learning from human preferences (OpenAI)</a></li><li><a href="https://arxiv.org/abs/2203.02155" target="_blank" rel="noreferrer">Training language models to follow instructions with human feedback (InstructGPT Paper)</a></li><li><a href="https://arxiv.org/abs/2212.08073" target="_blank" rel="noreferrer">Constitutional AI: Harmlessness from AI Feedback (Anthropic)</a></li></ul>',15)])])}const b=t(o,[["render",i]]);export{f as __pageData,b as default};
